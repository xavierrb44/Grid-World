{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Review gridworld_terminal_TD_helper.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOkc5kqoEDi9gnGqK5IRu2R"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8GDsm_jpUCoE","executionInfo":{"status":"ok","timestamp":1616515333676,"user_tz":-60,"elapsed":1355,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}}},"source":["'''\n","gridworld_terminal_TD.py\n","'''\n","\n","# libraries\n","import numpy as np\n","import sys\n","from tqdm import tqdm\n","\n","# state indexing: (x,y) -- for 4x4 grid\n","# [\n","#     (0,0), (0,1), (0,2), (0,3)\n","#     (1,0), (1,1), (1,2), (1,3)\n","#     (2,0), (2,1), (2,2), (2,3)\n","#     (3,0), (3,1), (3,2), (3,3)\n","# ]\n","\n","# left (0), up (1), right (2), down (3)\n","v_actions = [np.array([0, -1]),\n","            np.array([-1, 0]),\n","            np.array([0, 1]),\n","            np.array([1, 0])]\n","\n","i_nActions = len(v_actions)\n","\n","GAMMA = 0.9\n","ALPHA = 0.25 #0.1 \n","\n","def is_terminal(state, ginDim):\n","    x, y = state\n","    return (x == 0 and y == 0) or (x == ginDim - 1 and y == ginDim - 1)\n","\n","def get_initial_state(ginDim):\n","    # print(\"ginDim:%s\" %ginDim)\n","    while True:\n","        state_i = np.random.choice(ginDim)\n","        state_j = np.random.choice(ginDim)\n","        if (state_i == 0 and state_j == 0) or (state_i == ginDim-1 and state_j == ginDim-1):\n","            continue\n","        else:\n","            break\n","    return state_i, state_j\n","\n","\n","def target_policy_agent(state, gvPolicy, action=None):\n","    # print(state, action)\n","    if action is None:\n","        # t_iActionID = np.random.choice(i_nActions)\n","        t_iActionID = np.random.choice(list(range(i_nActions)), p=gvPolicy)\n","        # return the selected action and the prob of selection\n","        return t_iActionID, v_actions[t_iActionID], gvPolicy[t_iActionID]\n","    else:\n","        # return the given action and the prob of selecting that action at given state\n","        return action, v_actions[action], gvPolicy[action]\n","\n","\n","def step(giSt, giAc, ginDim):\n","    giSt = np.array(giSt)\n","    next_state = (giSt + giAc).tolist()\n","    x, y = next_state\n","    reward = -1\n","\n","    # if you exit the grid, return the original state\n","    if x < 0 or x >= ginDim or y < 0 or y >= ginDim:\n","        next_state = giSt.tolist()\n","\n","    return next_state, reward\n","\n","def TD0_travel(ginDim, policy_agent, gvPolicy, initial_state=None, initial_action=None):\n","    \n","    if initial_state is None:\n","        # generate a random initial state\n","        state_i, state_j = get_initial_state(ginDim)\n","    else:\n","        state_i, state_j = initial_state\n","    \n","    # initial state\n","    state = (state_i, state_j)\n"," \n","    states_and_rewards = [(state, 0)]\n","    while is_terminal(state, ginDim) == False:\n","         # get new action\n","         i_action, action, prob = policy_agent(state, gvPolicy)\n","         state, reward = step(state, action, ginDim)\n","         states_and_rewards.append((state,reward))\n","\n","    return states_and_rewards\n","\n","\n","\n","\n","def tabular_TD0(gvPolicy, episodes=100, ginDim=4):\n","    '''\n","    implement Tabular TD(0) algorithm for estimating v_{\\pi} --- i.e. policy prediction/evaluation algorithm\n","    '''\n","    \n","    state_values = np.zeros((ginDim, ginDim))\n","\n","    V = {}\n","    for i in range(ginDim):\n","      for j in range(ginDim):\n","            V[(i,j)] = 0\n","\n","    initial_state = get_initial_state(ginDim)\n","\n","    # travel for several episodes\n","    for episode in tqdm(range(0, episodes)):\n","        \n","        #generate episode\n","        states_and_rewards = TD0_travel(ginDim, target_policy_agent, gvPolicy, initial_state)\n","        for t in range(len(states_and_rewards) - 1):\n","            s, _ = states_and_rewards[t]\n","            s2, r = states_and_rewards[t+1]\n","            #update V[s] as we experience it\n","            x,y = s\n","            x2,y2 = s2\n","            V[(x,y)] = V[(x,y)] + ALPHA * (r + GAMMA*V[(x2,y2)] - V[(x,y)])\n","\n","    for i in range(ginDim):\n","      for j in range(ginDim):\n","        state_values[(i,j)] = V[(i,j)]\n","\n","    return episodes, np.around(state_values,1)\n","\n","\n","\n","def max_dict(d):\n","    '''\n","    :param d: dictionary\n","    :return: returns argmax (key) and max (value)\n","    '''\n","    max_key = None\n","    max_val = float('-inf')\n","    for k,v in d.items():\n","        if v > max_val:\n","            max_val = v\n","            max_key = k\n","    return max_key, max_val\n","\n","\n","def q_learning_algo(episodes=100, ginDim=4):\n","    '''\n","    implement Q-Learning algorithm\n","    '''\n","    state_values = np.zeros((ginDim, ginDim))\n","    best_actions = np.zeros((ginDim, ginDim))\n","\n","    gvPolicy = [0.25 ,0.25 ,0.25 ,0.25]\n","\n","    #initialize Q\n","    Q = {}\n","    for i in range(ginDim):\n","        for j in range(ginDim):\n","            Q[(i,j)] = {}\n","            for i_action, action in enumerate(v_actions): \n","                Q[(i,j)][i_action] = 0\n","    \n","    initial_state = get_initial_state(ginDim)\n","\n","    # travel for several episodes\n","    for episode in tqdm(range(0, episodes)):\n","\n","        #generate episode\n","        s = initial_state\n","        i_a, a, prob = target_policy_agent(s, gvPolicy)\n","\n","        while is_terminal(s, ginDim) == False:\n","            # get next step\n","            s2, r = step(s, a, ginDim)\n","\n","            #we will update Q(s,a) AS we experience the episode\n","            x,y = s\n","            old_qsa = Q[(x,y)][i_a]\n","\n","            #Q learning will use max[a'] Q(s',a') in our update\n","            x2,y2 = s2\n","            i_a2, max_q_s2a2 = max_dict(Q[(x2,y2)])\n","            Q[(x,y)][i_a] = Q[(x,y)][i_a] + ALPHA*(r + GAMMA*max_q_s2a2 - Q[(x,y)][i_a])\n","\n","            #next state becomes current state\n","            s = s2\n","            i_a = i_a2\n","            a = v_actions[i_a2]\n","    \n","    # determine policy from Q* and find V* from Q*\n","    policy = {}\n","    V = {}\n","    for i in range(ginDim):\n","      for j in range(ginDim):    \n","        s = (i,j)\n","        i_a, max_q = max_dict(Q[s])\n","        # way 1\n","        policy[s] = i_a\n","        V[s] = max_q\n","        #way 2\n","        best_actions[s] = i_a\n","        state_values[s] = max_q\n","\n","    return episodes, np.around(state_values,1), best_actions\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LAMmRXqrmCG","executionInfo":{"status":"ok","timestamp":1616515246978,"user_tz":-60,"elapsed":892,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}}},"source":["import math\n","import matplotlib.pyplot as plt\n","from matplotlib.table import Table\n","\n","def draw_policy(gdicPolicy, gsFigName):\n","    '''\n","    gdicPolicy: dictionary of values with cell coordinates as keys, and taken actions at the cells as values \n","    (0, 0) [0 1 2 3]\n","    (0, 1) [0 1 2 3]\n","    ...\n","\n","    gsFigName: output file name for visualization (e.g. gridworld_opt_policy_VI.png)\n","    '''\n","\n","    # left, up, right, down\n","    ACTIONS = [np.array([0, -1]),\n","            np.array([-1, 0]),\n","            np.array([0, 1]),\n","            np.array([1, 0])]\n","\n","    ACTIONS_FIGS=[ '←', '↑', '→', '↓']\n","\n","    fig, ax = plt.subplots()\n","    ax.set_axis_off()\n","    tb = Table(ax, bbox=[0, 0, 1, 1])\n","\n","\n","    dic_policy = dict(np.ndenumerate(gdicPolicy))\n","\n","    nrows, ncols = int(math.sqrt(len(dic_policy))), int(math.sqrt(len(dic_policy)))\n","    width, height = 1.0 / ncols, 1.0 / nrows\n","\n","    # Add cells\n","    # for (i, j), val in np.ndenumerate(gvOptValues):\n","    for cell, pol in dic_policy.items():\n","        \n","        #val=''\n","        #for ba in pol:\n","        #    val+=ACTIONS_FIGS[int(ba)]\n","        \n","        val = ACTIONS_FIGS[int(pol)]\n","\n","        i = cell[0]\n","        j = cell[1]\n","        \n","      \n","        tb.add_cell(i, j, width, height, text=val,\n","                loc='center', facecolor='white')\n","\n","    # Row and column labels...\n","    for i in range(int(math.sqrt(len(dic_policy)))):\n","        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n","                    edgecolor='none', facecolor='none')\n","        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n","                   edgecolor='none', facecolor='none')\n","\n","    ax.add_table(tb)\n","\n","    plt.savefig(gsFigName)\n","    plt.close()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwXUKkTti9BW","executionInfo":{"status":"ok","timestamp":1616515481130,"user_tz":-60,"elapsed":7498,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"be3747d5-145a-4819-a458-dbd028dc6660"},"source":["iterCt, policy_values = tabular_TD0([0.25 ,0.25 ,0.25 ,0.25], \n","                                    episodes=10000, ginDim=4)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100%|██████████| 10000/10000 [00:06<00:00, 1564.29it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyWkFouDkB9a","executionInfo":{"status":"ok","timestamp":1616515483118,"user_tz":-60,"elapsed":667,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"5e201975-7a77-4652-c77c-13b5a18123fc"},"source":["iterCt"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"T7--7fWB1NHG","executionInfo":{"status":"ok","timestamp":1616515499259,"user_tz":-60,"elapsed":642,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"4b5526b8-e2fd-45f4-9874-fdf51c0246db","colab":{"base_uri":"https://localhost:8080/"}},"source":["policy_values #[10000]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0. , -4.6, -7.5, -8.4],\n","       [-4.4, -6. , -7.2, -7.3],\n","       [-7. , -7.2, -7. , -5.2],\n","       [-7.6, -6.9, -5. ,  0. ]])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmrPYbDekDzj","executionInfo":{"status":"ok","timestamp":1616515349660,"user_tz":-60,"elapsed":1157,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"d61960ec-c2c6-4eb0-a732-e1cdf6258558"},"source":["policy_values #[1000]"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0. , -4.6, -6.5, -8. ],\n","       [-4.8, -6.5, -7. , -7.3],\n","       [-7. , -6.9, -6.7, -5.8],\n","       [-7.5, -7.1, -5.8,  0. ]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1doHMDR9dV9","executionInfo":{"status":"ok","timestamp":1616515274184,"user_tz":-60,"elapsed":889,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"78e56c72-6a77-4cd7-8e3c-d5dd8c6a4c31"},"source":["iterCt, policy_values, policy = q_learning_algo(episodes=1000, ginDim=4)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████| 1000/1000 [00:00<00:00, 17439.86it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnxH-aqaBQKC","executionInfo":{"status":"ok","timestamp":1616515276839,"user_tz":-60,"elapsed":1172,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"a7ed7f88-8781-4c7e-b4dc-b670ae3e5edd"},"source":["iterCt"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKFZQe3mBRsj","executionInfo":{"status":"ok","timestamp":1616515278908,"user_tz":-60,"elapsed":685,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"e7af0e7f-e9c7-46e7-d83c-6ad40b637125"},"source":["policy_values"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0. , -1. , -1.9, -2. ],\n","       [-1. , -1.9, -2.7, -1.9],\n","       [-1.9, -2.7, -1.9, -1. ],\n","       [-2. , -1.9, -1. ,  0. ]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6usWtVuPBUsH","executionInfo":{"status":"ok","timestamp":1614508521160,"user_tz":-60,"elapsed":1881,"user":{"displayName":"xavier rb44","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhLJ4vqc490M8w-LnILeR27p1LZN0WVvxkAktT1=s64","userId":"07655097380465990673"}},"outputId":"4b4ef7c2-ee57-4025-810f-7fd601abbd7d"},"source":["policy"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 3., 0.],\n","       [1., 1., 2., 1.],\n","       [1., 3., 0., 3.],\n","       [3., 0., 2., 0.]])"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"c25aBOCyMRBn"},"source":["s_fig_name = \"policy_TD_q_learning_algo_v1.png\"\n","draw_policy(policy, s_fig_name)"],"execution_count":null,"outputs":[]}]}